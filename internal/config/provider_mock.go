package config

import (
	"github.com/charmbracelet/crush/internal/fur/provider"
)

// MockProviders returns a mock list of providers for testing.
// This avoids making API calls during tests and provides consistent test data.
func MockProviders() []provider.Provider {
	return []provider.Provider{
		{
			Name:                "Anthropic",
			ID:                  provider.InferenceProviderAnthropic,
			APIKey:              "$ANTHROPIC_API_KEY",
			APIEndpoint:         "$ANTHROPIC_API_ENDPOINT",
			Type:                provider.TypeAnthropic,
			DefaultLargeModelID: "claude-3-opus",
			DefaultSmallModelID: "claude-3-haiku",
			Models: []provider.Model{
				{
					ID:                 "claude-3-opus",
					Name:               "Claude 3 Opus",
					CostPer1MIn:        15.0,
					CostPer1MOut:       75.0,
					CostPer1MInCached:  18.75,
					CostPer1MOutCached: 1.5,
					ContextWindow:      200000,
					DefaultMaxTokens:   4096,
					CanReason:          false,
					SupportsImages:     true,
				},
				{
					ID:                 "claude-3-haiku",
					Name:               "Claude 3 Haiku",
					CostPer1MIn:        0.25,
					CostPer1MOut:       1.25,
					CostPer1MInCached:  0.3,
					CostPer1MOutCached: 0.03,
					ContextWindow:      200000,
					DefaultMaxTokens:   4096,
					CanReason:          false,
					SupportsImages:     true,
				},
				{
					ID:                 "claude-3-5-sonnet-20241022",
					Name:               "Claude 3.5 Sonnet",
					CostPer1MIn:        3.0,
					CostPer1MOut:       15.0,
					CostPer1MInCached:  3.75,
					CostPer1MOutCached: 0.3,
					ContextWindow:      200000,
					DefaultMaxTokens:   8192,
					CanReason:          false,
					SupportsImages:     true,
				},
				{
					ID:                 "claude-3-5-haiku-20241022",
					Name:               "Claude 3.5 Haiku",
					CostPer1MIn:        0.8,
					CostPer1MOut:       4.0,
					CostPer1MInCached:  1.0,
					CostPer1MOutCached: 0.08,
					ContextWindow:      200000,
					DefaultMaxTokens:   8192,
					CanReason:          false,
					SupportsImages:     true,
				},
			},
		},
		{
			Name:                "OpenAI",
			ID:                  provider.InferenceProviderOpenAI,
			APIKey:              "$OPENAI_API_KEY",
			APIEndpoint:         "$OPENAI_API_ENDPOINT",
			Type:                provider.TypeOpenAI,
			DefaultLargeModelID: "gpt-4",
			DefaultSmallModelID: "gpt-3.5-turbo",
			Models: []provider.Model{
				{
					ID:                 "gpt-4",
					Name:               "GPT-4",
					CostPer1MIn:        30.0,
					CostPer1MOut:       60.0,
					CostPer1MInCached:  0.0,
					CostPer1MOutCached: 0.0,
					ContextWindow:      8192,
					DefaultMaxTokens:   4096,
					CanReason:          false,
					SupportsImages:     false,
				},
				{
					ID:                 "gpt-3.5-turbo",
					Name:               "GPT-3.5 Turbo",
					CostPer1MIn:        1.0,
					CostPer1MOut:       2.0,
					CostPer1MInCached:  0.0,
					CostPer1MOutCached: 0.0,
					ContextWindow:      4096,
					DefaultMaxTokens:   4096,
					CanReason:          false,
					SupportsImages:     false,
				},
				{
					ID:                 "gpt-4-turbo",
					Name:               "GPT-4 Turbo",
					CostPer1MIn:        10.0,
					CostPer1MOut:       30.0,
					CostPer1MInCached:  0.0,
					CostPer1MOutCached: 0.0,
					ContextWindow:      128000,
					DefaultMaxTokens:   4096,
					CanReason:          false,
					SupportsImages:     true,
				},
				{
					ID:                 "gpt-4o",
					Name:               "GPT-4o",
					CostPer1MIn:        2.5,
					CostPer1MOut:       10.0,
					CostPer1MInCached:  0.0,
					CostPer1MOutCached: 1.25,
					ContextWindow:      128000,
					DefaultMaxTokens:   16384,
					CanReason:          false,
					SupportsImages:     true,
				},
				{
					ID:                 "gpt-4o-mini",
					Name:               "GPT-4o-mini",
					CostPer1MIn:        0.15,
					CostPer1MOut:       0.6,
					CostPer1MInCached:  0.0,
					CostPer1MOutCached: 0.075,
					ContextWindow:      128000,
					DefaultMaxTokens:   16384,
					CanReason:          false,
					SupportsImages:     true,
				},
				{
					ID:                     "o1-preview",
					Name:                   "o1-preview",
					CostPer1MIn:            15.0,
					CostPer1MOut:           60.0,
					CostPer1MInCached:      0.0,
					CostPer1MOutCached:     0.0,
					ContextWindow:          128000,
					DefaultMaxTokens:       32768,
					CanReason:              true,
					HasReasoningEffort:     true,
					DefaultReasoningEffort: "medium",
					SupportsImages:         true,
				},
				{
					ID:                     "o1-mini",
					Name:                   "o1-mini",
					CostPer1MIn:            3.0,
					CostPer1MOut:           12.0,
					CostPer1MInCached:      0.0,
					CostPer1MOutCached:     0.0,
					ContextWindow:          128000,
					DefaultMaxTokens:       65536,
					CanReason:              true,
					HasReasoningEffort:     true,
					DefaultReasoningEffort: "medium",
					SupportsImages:         true,
				},
			},
		},
		{
			Name:                "Google Gemini",
			ID:                  provider.InferenceProviderGemini,
			APIKey:              "$GEMINI_API_KEY",
			APIEndpoint:         "$GEMINI_API_ENDPOINT",
			Type:                provider.TypeGemini,
			DefaultLargeModelID: "gemini-2.5-pro",
			DefaultSmallModelID: "gemini-2.5-flash",
			Models: []provider.Model{
				{
					ID:                 "gemini-2.5-pro",
					Name:               "Gemini 2.5 Pro",
					CostPer1MIn:        1.25,
					CostPer1MOut:       10.0,
					CostPer1MInCached:  1.625,
					CostPer1MOutCached: 0.31,
					ContextWindow:      1048576,
					DefaultMaxTokens:   65536,
					CanReason:          true,
					SupportsImages:     true,
				},
				{
					ID:                 "gemini-2.5-flash",
					Name:               "Gemini 2.5 Flash",
					CostPer1MIn:        0.3,
					CostPer1MOut:       2.5,
					CostPer1MInCached:  0.3833,
					CostPer1MOutCached: 0.075,
					ContextWindow:      1048576,
					DefaultMaxTokens:   65535,
					CanReason:          true,
					SupportsImages:     true,
				},
			},
		},
		{
			Name:                "xAI",
			ID:                  provider.InferenceProviderXAI,
			APIKey:              "$XAI_API_KEY",
			APIEndpoint:         "https://api.x.ai/v1",
			Type:                provider.TypeXAI,
			DefaultLargeModelID: "grok-beta",
			DefaultSmallModelID: "grok-beta",
			Models: []provider.Model{
				{
					ID:               "grok-beta",
					Name:             "Grok Beta",
					CostPer1MIn:      5.0,
					CostPer1MOut:     15.0,
					ContextWindow:    131072,
					DefaultMaxTokens: 4096,
					CanReason:        false,
					SupportsImages:   true,
				},
			},
		},
		{
			Name:                "OpenRouter",
			ID:                  provider.InferenceProviderOpenRouter,
			APIKey:              "$OPENROUTER_API_KEY",
			APIEndpoint:         "https://openrouter.ai/api/v1",
			Type:                provider.TypeOpenAI,
			DefaultLargeModelID: "anthropic/claude-3.5-sonnet",
			DefaultSmallModelID: "anthropic/claude-3.5-haiku",
			Models: []provider.Model{
				{
					ID:               "anthropic/claude-3.5-sonnet",
					Name:             "Claude 3.5 Sonnet",
					CostPer1MIn:      3.0,
					CostPer1MOut:     15.0,
					ContextWindow:    200000,
					DefaultMaxTokens: 8192,
					CanReason:        false,
					SupportsImages:   true,
				},
				{
					ID:               "anthropic/claude-3.5-haiku",
					Name:             "Claude 3.5 Haiku",
					CostPer1MIn:      0.8,
					CostPer1MOut:     4.0,
					ContextWindow:    200000,
					DefaultMaxTokens: 8192,
					CanReason:        false,
					SupportsImages:   true,
				},
			},
		},
	}
}
